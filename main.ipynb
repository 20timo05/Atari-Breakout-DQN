{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"gymnasium[atari, accept-rom-license, other]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.play import play\n",
    "from gymnasium.vector import SyncVectorEnv, AsyncVectorEnv\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from utils import DQN, ReplayMemory\n",
    "\n",
    "from hyperparameters import (\n",
    "    NUM_ENVS,\n",
    "    BATCH_SIZE,\n",
    "    LEARNING_RATE,\n",
    "    TRAIN_STEPS,\n",
    "    MIN_REPLAY_MEMORY_SIZE,\n",
    "    MAX_REPLAY_MEMORY_SIZE,\n",
    "    UPDATE_TARGET_NETWORK,\n",
    "    MAX_EPSILON,\n",
    "    MIN_EPSILON,\n",
    "    EPSILON_DECAY,\n",
    "    PRINT_LOGS_STEPS,\n",
    "    DEVICE,\n",
    "    SAVE_PATH,\n",
    "    SAVE_INTERVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make(\"ALE/Breakout-v5\")\n",
    "        env = AtariPreprocessing(\n",
    "            env,\n",
    "            frame_skip=1,  # ALE/Breakout-v5 already uses frame_skip=4\n",
    "            screen_size=84,\n",
    "            grayscale_obs=True,\n",
    "            noop_max=30,\n",
    "        )\n",
    "        env = FrameStack(env, 4)\n",
    "        return env\n",
    "\n",
    "    return _init\n",
    "\n",
    "\n",
    "if DEVICE == \"cpu\":\n",
    "    envs = SyncVectorEnv([make_env() for _ in range(NUM_ENVS)])\n",
    "else:\n",
    "    envs = AsyncVectorEnv([make_env() for _ in range(NUM_ENVS)])\n",
    "\n",
    "\n",
    "policy_network = DQN((4, 84, 84), 4)\n",
    "policy_network.to(DEVICE)\n",
    "target_network = DQN((4, 84, 84), 4)\n",
    "target_network.to(DEVICE)\n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "target_network.eval()\n",
    "\n",
    "replay_memory = ReplayMemory(MAX_REPLAY_MEMORY_SIZE, DEVICE)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "summary_writer = SummaryWriter(\"./logs/atari_vanilla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Replay Buffer with random experiences\n",
    "states, _ = envs.reset()\n",
    "for _ in tqdm(range(MIN_REPLAY_MEMORY_SIZE)):\n",
    "  actions = envs.action_space.sample()\n",
    "  result = envs.step(actions)\n",
    "  new_states, rewards, terminated, truncated, _ = result\n",
    "  \n",
    "  for state, action, reward, new_state, ter, trunc in zip(states, actions, rewards, new_states, terminated, truncated):\n",
    "    experience = (state, action, reward, new_state, ter or trunc)\n",
    "    replay_memory.append(experience)\n",
    "\n",
    "  states = new_states\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\q603178\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\q603178\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/10000) 0.0000\n",
      "Episode 200/10000) 0.0000\n",
      "Episode 300/10000) 0.0000\n",
      "Episode 400/10000) 0.0000\n",
      "Episode 500/10000) 0.0000\n",
      "Episode 600/10000) 0.0000\n",
      "Episode 700/10000) 0.0000\n",
      "Episode 800/10000) 1.2500\n",
      "Episode 900/10000) 1.2500\n",
      "Episode 1000/10000) 1.2500\n",
      "Episode 1100/10000) 1.2500\n",
      "Episode 1200/10000) 1.0000\n",
      "Episode 1300/10000) 0.8333\n",
      "Episode 1400/10000) 0.8571\n",
      "Episode 1500/10000) 0.8571\n",
      "Episode 1600/10000) 1.0000\n",
      "Episode 1700/10000) 0.8889\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minterp(step, [\u001b[38;5;241m0\u001b[39m, EPSILON_DECAY], [MAX_EPSILON, MIN_EPSILON])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# perform action and store experience in replay Memory\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# perform and observe\u001b[39;00m\n\u001b[0;32m     16\u001b[0m new_states, rewards, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "File \u001b[1;32mc:\\Users\\q603178\\Desktop\\Learning\\Q-Learning\\Atari Breakout\\utils.py:40\u001b[0m, in \u001b[0;36mDQN.act\u001b[1;34m(self, states, epsilon)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, states, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m   states_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(states, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     42\u001b[0m   qvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(states_tensor)\n\u001b[0;32m     43\u001b[0m   actions \u001b[38;5;241m=\u001b[39m qvalues\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy_network.train()\n",
    "    \n",
    "reward_logs = [[] for _ in range(NUM_ENVS)]\n",
    "total_reward = [0 for _ in range(NUM_ENVS)]\n",
    "loss_logs = []\n",
    "\n",
    "states, _ = envs.reset()\n",
    "\n",
    "step = 0\n",
    "while step < TRAIN_STEPS:\n",
    "  epsilon = np.interp(step, [0, EPSILON_DECAY], [MAX_EPSILON, MIN_EPSILON])\n",
    "\n",
    "  # perform action and store experience in replay Memory\n",
    "  actions = policy_network.act(states, epsilon)\n",
    "\n",
    "  # perform and observe\n",
    "  new_states, rewards, terminated, truncated, _ = envs.step(actions)\n",
    "  \n",
    "  for i, (state, action, reward, new_state, ter, trunc) in enumerate(zip(states, actions, rewards, new_states, terminated, truncated)):\n",
    "    experience = (state, action, reward, new_state, ter or trunc)\n",
    "    replay_memory.append(experience)\n",
    "    \n",
    "    total_reward[i] += reward\n",
    "    if ter or trunc:\n",
    "      reward_logs[i].append(total_reward[i])\n",
    "      total_reward[i] = 0\n",
    "      \n",
    "  states = new_states\n",
    "\n",
    "  \"\"\" TRAIN DQN \"\"\"\n",
    "  batch = replay_memory.sample(BATCH_SIZE)\n",
    "  loss = policy_network.compute_loss(batch, target_network)\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  loss_logs.append(loss.item())\n",
    "\n",
    "  \n",
    "  if step % UPDATE_TARGET_NETWORK < NUM_ENVS:\n",
    "    target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "\n",
    "  if step % PRINT_LOGS_STEPS < NUM_ENVS and step != 0:\n",
    "    avg_reward = np.mean([item for sublist in reward_logs for item in sublist[-PRINT_LOGS_STEPS:]])\n",
    "    avg_reward = avg_reward if not np.isnan(avg_reward) else 0\n",
    "\n",
    "    avg_loss = np.mean(loss_logs[-PRINT_LOGS_STEPS:])\n",
    "\n",
    "    print(f\"Episode {step}/{TRAIN_STEPS}) reward: {avg_reward:.4f} - loss: {avg_loss:.4f}\")\n",
    "\n",
    "    summary_writer.add_scalar(\"AVG Reward\", avg_reward, global_step=step)\n",
    "  \n",
    "  if step % SAVE_INTERVAL < NUM_ENVS and step != 0:\n",
    "    print(\"Saving...\")\n",
    "    torch.save(policy_network.state_dict(), SAVE_PATH)\n",
    "  \n",
    "  step += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
